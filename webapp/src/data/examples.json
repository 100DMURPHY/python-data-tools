{
  "load_csv_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### CSV Loading with Pandas\n# This example demonstrates how to load a CSV file, with self-healing data generation.\n\n# %%\nimport pandas as pd\nimport pathlib\n\n# Self-healing: Generate data if missing for portability\npath = pathlib.Path(\"data.csv\")\nif not path.exists():\n    pd.DataFrame({\n        \"id\": [1, 2, 3],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"city\": [\"NY\", \"SF\", \"LA\"]\n    }).to_csv(path, index=False)\n\n# %%\n# Basic CSV loading\ndf = pd.read_csv(path)\nprint(f\"Pandas loaded {len(df)} rows:\")\nprint(df.head())\n\n# With options\ndf = pd.read_csv(\n    path,\n    sep=\",\",\n    header=0,\n    dtype={\"id\": int, \"name\": str},\n)",
    "language": "python",
    "output": "Pandas loaded 3 rows:\n   id     name city\n0   1    Alice   NY\n1   2      Bob   SF\n2   3  Charlie   LA\n"
  },
  "load_csv_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n# ]\n# ///\n# %% [markdown]\n# ### CSV Loading with Polars\n# Demonstrates high-performance CSV loading.\n\n# %%\nimport polars as pl\nimport pathlib\n\n# Self-healing: Generate data if missing for portability\npath = pathlib.Path(\"data.csv\")\nif not path.exists():\n    pl.DataFrame({\n        \"id\": [1, 2, 3],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"city\": [\"NY\", \"SF\", \"LA\"]\n    }).write_csv(path)\n\n# %%\n# Basic CSV loading (eager)\ndf = pl.read_csv(path)\nprint(f\"Polars loaded {len(df)} rows:\")\nprint(df.head())\n\n# Lazy loading (recommended for large files)\nlf = pl.scan_csv(path)\ndf = lf.collect()",
    "language": "python",
    "output": "Polars loaded 3 rows:\nshape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 name    \u2506 city \u2502\n\u2502 --- \u2506 ---     \u2506 ---  \u2502\n\u2502 i64 \u2506 str     \u2506 str  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 Alice   \u2506 NY   \u2502\n\u2502 2   \u2506 Bob     \u2506 SF   \u2502\n\u2502 3   \u2506 Charlie \u2506 LA   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "load_csv_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### CSV Loading with DuckDB\n# Using SQL to query CSV files directly.\n\n# %%\nimport duckdb\nimport pandas as pd\nimport pathlib\n\n# Self-healing: Generate data if missing for portability\npath = pathlib.Path(\"data.csv\")\nif not path.exists():\n    pd.DataFrame({\n        \"id\": [1, 2, 3],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"city\": [\"NY\", \"SF\", \"LA\"]\n    }).to_csv(path, index=False)\n\n# %%\n# Basic CSV loading via SQL\nduckdb.sql(f\"SELECT * FROM '{path}'\").show()",
    "language": "python",
    "output": "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  id   \u2502  name   \u2502  city   \u2502\n\u2502 int64 \u2502 varchar \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     1 \u2502 Alice   \u2502 NY      \u2502\n\u2502     2 \u2502 Bob     \u2502 SF      \u2502\n\u2502     3 \u2502 Charlie \u2502 LA      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
  },
  "load_csv_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### CSV Loading with BigQuery\n# Demonstrates loading local data to BigQuery.\n\n# %%\nimport unittest.mock as mock\nfrom google.cloud import bigquery\nimport pandas as pd\nimport pathlib\n\n# Self-healing: Generate data if missing for portability\npath = pathlib.Path(\"data.csv\")\nif not path.exists():\n    pd.DataFrame({\n        \"id\": [1, 2, 3],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"city\": [\"NY\", \"SF\", \"LA\"]\n    }).to_csv(path, index=False)\n\n# %%\n# Mock the client for CI/verification purposes\nclient = mock.MagicMock(spec=bigquery.Client)\n\n# Initialize client (Mocked for verification)\n# client = bigquery.Client()\n\n# Load CSV from local file to BigQuery table\ntable_id = \"project.dataset.table_name\"\n\njob_config = bigquery.LoadJobConfig(\n    source_format=bigquery.SourceFormat.CSV,\n    skip_leading_rows=1,\n    autodetect=True,\n)\n\n# %%\n# Mocking the load_table_from_file behavior\njob = client.load_table_from_file(None, table_id, job_config=job_config)\njob.result() \n\nprint(f\"Mock BigQuery load triggered for {table_id}\")",
    "language": "python",
    "output": "Mock BigQuery load triggered for project.dataset.table_name\n"
  },
  "load_parquet_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\n# %% [markdown]\n# ### Parquet Loading with Pandas\n# Columnar data handling with PyArrow engine.\n\n# %%\nimport pandas as pd\nimport pathlib\n\n# Self-healing: Generate data if missing\npath = pathlib.Path(\"data.parquet\")\nif not path.exists():\n    pd.DataFrame({\n        \"id\": range(100),\n        \"val\": range(100, 200)\n    }).to_parquet(path)\n\n# %%\n# Load Parquet\ndf = pd.read_parquet(path)\nprint(f\"Pandas loaded Parquet with {len(df)} rows.\")\nprint(df.head())",
    "language": "python",
    "output": "Pandas loaded Parquet with 100 rows.\n   id  val\n0   0  100\n1   1  101\n2   2  102\n3   3  103\n4   4  104\n"
  },
  "load_parquet_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\n# %% [markdown]\n# ### Parquet Loading with Polars\n# Fast, multithreaded Parquet reading.\n\n# %%\nimport polars as pl\nimport pathlib\nimport pandas as pd # For initial data gen\n\n# Self-healing\npath = pathlib.Path(\"data.parquet\")\nif not path.exists():\n    pd.DataFrame({\"id\": range(100)}).to_parquet(path)\n\n# %%\n# Load Parquet (Eager)\ndf = pl.read_parquet(path)\nprint(f\"Polars loaded {len(df)} rows.\")\n\n# Scan Parquet (Lazy - Recommended for Large Data)\nquery = pl.scan_parquet(path).select([\"id\"]).limit(5)\nprint(\"Lazy scan result:\")\nprint(query.collect())",
    "language": "python",
    "output": "Polars loaded 100 rows.\nLazy scan result:\nshape: (5, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2502\n\u2502 --- \u2502\n\u2502 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2502\n\u2502 1   \u2502\n\u2502 2   \u2502\n\u2502 3   \u2502\n\u2502 4   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "load_parquet_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\n# %% [markdown]\n# ### Parquet Loading with DuckDB\n# Direct SQL queries on Parquet files.\n\n# %%\nimport duckdb\nimport pathlib\nimport pandas as pd\n\n# Self-healing\npath = pathlib.Path(\"data.parquet\")\nif not path.exists():\n    pd.DataFrame({\"id\": range(100)}).to_parquet(path)\n\n# %%\n# Query Parquet directly\nduckdb.sql(f\"SELECT COUNT(*) FROM '{path}'\").show()\n\n# Read as relation\nrel = duckdb.read_parquet(str(path))\nrel.limit(5).show()",
    "language": "python",
    "output": "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 count_star() \u2502\n\u2502    int64     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502          100 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  id   \u2502  val  \u2502\n\u2502 int64 \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     0 \u2502   100 \u2502\n\u2502     1 \u2502   101 \u2502\n\u2502     2 \u2502   102 \u2502\n\u2502     3 \u2502   103 \u2502\n\u2502     4 \u2502   104 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
  },
  "load_parquet_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\n# %% [markdown]\n# ### Parquet Loading with BigQuery\n# Demonstrates loading Parquet data into BigQuery tables.\n\n# %%\nimport unittest.mock as mock\nfrom google.cloud import bigquery\nimport pandas as pd\nimport pathlib\n\n# Self-healing\npath = pathlib.Path(\"data.parquet\")\nif not path.exists():\n    pd.DataFrame({\"id\": range(100)}).to_parquet(path)\n\n# %%\n# Mock the client\nclient = mock.MagicMock(spec=bigquery.Client)\n\n# Load configuration\ntable_id = \"project.dataset.parquet_table\"\njob_config = bigquery.LoadJobConfig(\n    source_format=bigquery.SourceFormat.PARQUET,\n    write_disposition=\"WRITE_TRUNCATE\",\n)\n\n# %%\n# Trigger load\nwith open(path, \"rb\") as source_file:\n    job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n    job.result()\n\nprint(f\"Mock BigQuery Parquet load triggered for {table_id}\")",
    "language": "python",
    "output": "Mock BigQuery Parquet load triggered for project.dataset.parquet_table\n"
  }
}