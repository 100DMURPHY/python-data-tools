{
  "load_csv_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### CSV Loading with Pandas\n# This example demonstrates how to load a CSV file, with self-healing data generation.\n\n# %%\nimport pandas as pd\nimport pathlib\n\n# Self-healing: Generate data if missing for portability\npath = pathlib.Path(\"data.csv\")\nif not path.exists():\n    pd.DataFrame({\n        \"id\": [1, 2, 3],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"city\": [\"NY\", \"SF\", \"LA\"]\n    }).to_csv(path, index=False)\n\n# %%\n# Basic CSV loading\ndf = pd.read_csv(path)\nprint(f\"Pandas loaded {len(df)} rows:\")\nprint(df.head())\n\n# With options\ndf = pd.read_csv(\n    path,\n    sep=\",\",\n    header=0,\n    dtype={\"id\": int, \"name\": str},\n)",
    "language": "python",
    "output": "Pandas loaded 3 rows:\n   id     name city\n0   1    Alice   NY\n1   2      Bob   SF\n2   3  Charlie   LA\n"
  },
  "load_csv_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n# ]\n# ///\n# %% [markdown]\n# ### CSV Loading with Polars\n# Demonstrates high-performance CSV loading.\n\n# %%\nimport polars as pl\nimport pathlib\n\n# Self-healing: Generate data if missing for portability\npath = pathlib.Path(\"data.csv\")\nif not path.exists():\n    pl.DataFrame({\n        \"id\": [1, 2, 3],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"city\": [\"NY\", \"SF\", \"LA\"]\n    }).write_csv(path)\n\n# %%\n# Basic CSV loading (eager)\ndf = pl.read_csv(path)\nprint(f\"Polars loaded {len(df)} rows:\")\nprint(df.head())\n\n# Lazy loading (recommended for large files)\nlf = pl.scan_csv(path)\ndf = lf.collect()",
    "language": "python",
    "output": "Polars loaded 3 rows:\nshape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 name    \u2506 city \u2502\n\u2502 --- \u2506 ---     \u2506 ---  \u2502\n\u2502 i64 \u2506 str     \u2506 str  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 Alice   \u2506 NY   \u2502\n\u2502 2   \u2506 Bob     \u2506 SF   \u2502\n\u2502 3   \u2506 Charlie \u2506 LA   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "load_csv_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### CSV Loading with DuckDB\n# Using SQL to query CSV files directly.\n\n# %%\nimport duckdb\nimport pandas as pd\nimport pathlib\n\n# Self-healing: Generate data if missing for portability\npath = pathlib.Path(\"data.csv\")\nif not path.exists():\n    pd.DataFrame({\n        \"id\": [1, 2, 3],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"city\": [\"NY\", \"SF\", \"LA\"]\n    }).to_csv(path, index=False)\n\n# %%\n# Basic CSV loading via SQL\nduckdb.sql(f\"SELECT * FROM '{path}'\").show()",
    "language": "python",
    "output": "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  id   \u2502  name   \u2502  city   \u2502\n\u2502 int64 \u2502 varchar \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     1 \u2502 Alice   \u2502 NY      \u2502\n\u2502     2 \u2502 Bob     \u2502 SF      \u2502\n\u2502     3 \u2502 Charlie \u2502 LA      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
  },
  "load_csv_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### CSV Loading with BigQuery\n# Demonstrates loading local data to BigQuery.\n\n# %%\nimport unittest.mock as mock\nfrom google.cloud import bigquery\nimport pandas as pd\nimport pathlib\n\n# Self-healing: Generate data if missing for portability\npath = pathlib.Path(\"data.csv\")\nif not path.exists():\n    pd.DataFrame({\n        \"id\": [1, 2, 3],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"city\": [\"NY\", \"SF\", \"LA\"]\n    }).to_csv(path, index=False)\n\n# %%\n# Mock the client for CI/verification purposes\nclient = mock.MagicMock(spec=bigquery.Client)\n\n# Initialize client (Mocked for verification)\n# client = bigquery.Client()\n\n# Load CSV from local file to BigQuery table\ntable_id = \"project.dataset.table_name\"\n\njob_config = bigquery.LoadJobConfig(\n    source_format=bigquery.SourceFormat.CSV,\n    skip_leading_rows=1,\n    autodetect=True,\n)\n\n# %%\n# Mocking the load_table_from_file behavior\njob = client.load_table_from_file(None, table_id, job_config=job_config)\njob.result() \n\nprint(f\"Mock BigQuery load triggered for {table_id}\")",
    "language": "python",
    "output": "Mock BigQuery load triggered for project.dataset.table_name\n"
  },
  "load_parquet_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\n# %% [markdown]\n# ### Parquet Loading with Pandas\n# Columnar data handling with PyArrow engine.\n\n# %%\nimport pandas as pd\nimport pathlib\n\n# Self-healing: Generate data if missing\npath = pathlib.Path(\"data.parquet\")\nif not path.exists():\n    pd.DataFrame({\n        \"id\": range(100),\n        \"val\": range(100, 200)\n    }).to_parquet(path)\n\n# %%\n# Load Parquet\ndf = pd.read_parquet(path)\nprint(f\"Pandas loaded Parquet with {len(df)} rows.\")\nprint(df.head())",
    "language": "python",
    "output": "Pandas loaded Parquet with 100 rows.\n   id  val\n0   0  100\n1   1  101\n2   2  102\n3   3  103\n4   4  104\n"
  },
  "load_parquet_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\n# %% [markdown]\n# ### Parquet Loading with Polars\n# Fast, multithreaded Parquet reading.\n\n# %%\nimport polars as pl\nimport pathlib\nimport pandas as pd # For initial data gen\n\n# Self-healing\npath = pathlib.Path(\"data.parquet\")\nif not path.exists():\n    pd.DataFrame({\"id\": range(100)}).to_parquet(path)\n\n# %%\n# Load Parquet (Eager)\ndf = pl.read_parquet(path)\nprint(f\"Polars loaded {len(df)} rows.\")\n\n# Scan Parquet (Lazy - Recommended for Large Data)\nquery = pl.scan_parquet(path).select([\"id\"]).limit(5)\nprint(\"Lazy scan result:\")\nprint(query.collect())",
    "language": "python",
    "output": "Polars loaded 100 rows.\nLazy scan result:\nshape: (5, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2502\n\u2502 --- \u2502\n\u2502 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2502\n\u2502 1   \u2502\n\u2502 2   \u2502\n\u2502 3   \u2502\n\u2502 4   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "load_parquet_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\n# %% [markdown]\n# ### Parquet Loading with DuckDB\n# Direct SQL queries on Parquet files.\n\n# %%\nimport duckdb\nimport pathlib\nimport pandas as pd\n\n# Self-healing\npath = pathlib.Path(\"data.parquet\")\nif not path.exists():\n    pd.DataFrame({\"id\": range(100)}).to_parquet(path)\n\n# %%\n# Query Parquet directly\nduckdb.sql(f\"SELECT COUNT(*) FROM '{path}'\").show()\n\n# Read as relation\nrel = duckdb.read_parquet(str(path))\nrel.limit(5).show()",
    "language": "python",
    "output": "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 count_star() \u2502\n\u2502    int64     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502          100 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  id   \u2502  val  \u2502\n\u2502 int64 \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     0 \u2502   100 \u2502\n\u2502     1 \u2502   101 \u2502\n\u2502     2 \u2502   102 \u2502\n\u2502     3 \u2502   103 \u2502\n\u2502     4 \u2502   104 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
  },
  "load_parquet_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\n# %% [markdown]\n# ### Parquet Loading with BigQuery\n# Demonstrates loading Parquet data into BigQuery tables.\n\n# %%\nimport unittest.mock as mock\nfrom google.cloud import bigquery\nimport pandas as pd\nimport pathlib\n\n# Self-healing\npath = pathlib.Path(\"data.parquet\")\nif not path.exists():\n    pd.DataFrame({\"id\": range(100)}).to_parquet(path)\n\n# %%\n# Mock the client\nclient = mock.MagicMock(spec=bigquery.Client)\n\n# Load configuration\ntable_id = \"project.dataset.parquet_table\"\njob_config = bigquery.LoadJobConfig(\n    source_format=bigquery.SourceFormat.PARQUET,\n    write_disposition=\"WRITE_TRUNCATE\",\n)\n\n# %%\n# Trigger load\nwith open(path, \"rb\") as source_file:\n    job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n    job.result()\n\nprint(f\"Mock BigQuery Parquet load triggered for {table_id}\")",
    "language": "python",
    "output": "Mock BigQuery Parquet load triggered for project.dataset.parquet_table\n"
  },
  "load_json_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### JSON & NDJSON Loading with Pandas\n# Handling standard JSON and Newline Delimited JSON (JSONL).\n\n# %%\nimport pandas as pd\nimport pathlib\nimport json\n\n# Self-healing: Generate standard JSON\njson_path = pathlib.Path(\"data.json\")\nif not json_path.exists():\n    data = [\n        {\"id\": 1, \"user\": \"alice\", \"meta\": {\"login\": \"2024-01-01\"}},\n        {\"id\": 2, \"user\": \"bob\", \"meta\": {\"login\": \"2024-01-02\"}}\n    ]\n    with open(json_path, \"w\") as f:\n        json.dump(data, f)\n\n# Self-healing: Generate NDJSON (Newline Delimited)\nndjson_path = pathlib.Path(\"data.jsonl\")\nif not ndjson_path.exists():\n    with open(ndjson_path, \"w\") as f:\n        f.write('{\"id\": 1, \"val\": 10}\\n')\n        f.write('{\"id\": 2, \"val\": 20}\\n')\n\n# %%\n# Load standard JSON\ndf_json = pd.read_json(json_path)\nprint(\"Standard JSON (Pandas):\")\nprint(df_json.head())\n\n# %%\n# Load NDJSON (lines=True)\ndf_ndjson = pd.read_json(ndjson_path, lines=True)\nprint(\"\\nNDJSON (lines=True):\")\nprint(df_ndjson.head())",
    "language": "python",
    "output": "Standard JSON (Pandas):\n   id   user                     meta\n0   1  alice  {'login': '2024-01-01'}\n1   2    bob  {'login': '2024-01-02'}\n\nNDJSON (lines=True):\n   id  val\n0   1   10\n1   2   20\n"
  },
  "load_json_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n# ]\n# ///\n# %% [markdown]\n# ### JSON & NDJSON Loading with Polars\n# Fast parsing for structured and semi-structured data.\n\n# %%\nimport polars as pl\nimport pathlib\nimport json\n\n# Self-healing: Generate standard JSON\njson_path = pathlib.Path(\"data.json\")\nif not json_path.exists():\n    data = [{\"id\": i, \"name\": f\"user_{i}\"} for i in range(5)]\n    with open(json_path, \"w\") as f:\n        json.dump(data, f)\n\n# Self-healing: Generate NDJSON\nndjson_path = pathlib.Path(\"data.jsonl\")\nif not ndjson_path.exists():\n    with open(ndjson_path, \"w\") as f:\n        for i in range(5):\n            f.write(json.dumps({\"id\": i, \"score\": i * 1.5}) + \"\\n\")\n\n# %%\n# Load standard JSON (Eager)\ndf_json = pl.read_json(json_path)\nprint(\"Standard JSON (Polars):\")\nprint(df_json)\n\n# %%\n# Load NDJSON (Fastest)\ndf_ndjson = pl.read_ndjson(ndjson_path)\nprint(\"\\nNDJSON (Polars):\")\nprint(df_ndjson)\n\n# Scan NDJSON (Lazy - Great for large logs)\n# df_lazy = pl.scan_ndjson(ndjson_path).collect()",
    "language": "python",
    "output": "Standard JSON (Polars):\nshape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 user  \u2506 meta           \u2502\n\u2502 --- \u2506 ---   \u2506 ---            \u2502\n\u2502 i64 \u2506 str   \u2506 struct[1]      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 alice \u2506 {\"2024-01-01\"} \u2502\n\u2502 2   \u2506 bob   \u2506 {\"2024-01-02\"} \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nNDJSON (Polars):\nshape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 val \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 10  \u2502\n\u2502 2   \u2506 20  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "load_json_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n# ]\n# ///\n# %% [markdown]\n# ### JSON Loading with DuckDB\n# Direct SQL queries on JSON and NDJSON files.\n\n# %%\nimport duckdb\nimport pathlib\nimport json\n\n# Self-healing\njson_path = pathlib.Path(\"data.json\")\nif not json_path.exists():\n    with open(json_path, \"w\") as f:\n        json.dump([{\"id\": 1, \"v\": \"a\"}, {\"id\": 2, \"v\": \"b\"}], f)\n\nndjson_path = pathlib.Path(\"data.jsonl\")\nif not ndjson_path.exists():\n    with open(ndjson_path, \"w\") as f:\n        f.write('{\"id\": 10, \"v\": \"X\"}\\n{\"id\": 20, \"v\": \"Y\"}\\n')\n\n# %%\n# Query JSON directly via SQL\nprint(\"Standard JSON via SQL:\")\nduckdb.sql(f\"SELECT * FROM read_json_auto('{json_path}')\").show()\n\n# Query NDJSON\nprint(\"\\nNDJSON via SQL:\")\nduckdb.sql(f\"SELECT * FROM read_json_auto('{ndjson_path}')\").show()",
    "language": "python",
    "output": "Standard JSON via SQL:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  id   \u2502  user   \u2502         meta          \u2502\n\u2502 int64 \u2502 varchar \u2502  struct(login date)   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     1 \u2502 alice   \u2502 {'login': 2024-01-01} \u2502\n\u2502     2 \u2502 bob     \u2502 {'login': 2024-01-02} \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\nNDJSON via SQL:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  id   \u2502  val  \u2502\n\u2502 int64 \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     1 \u2502    10 \u2502\n\u2502     2 \u2502    20 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
  },
  "load_json_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n# ]\n# ///\n# %% [markdown]\n# ### JSON Loading with BigQuery\n# Loading NDJSON data into BigQuery tables. Note: BigQuery requires NDJSON (Newline Delimited) for loading local files.\n\n# %%\nimport unittest.mock as mock\nfrom google.cloud import bigquery\nimport pathlib\nimport json\n\n# Self-healing: Generate NDJSON (Native to BigQuery load)\nndjson_path = pathlib.Path(\"data.jsonl\")\nif not ndjson_path.exists():\n    with open(ndjson_path, \"w\") as f:\n        f.write('{\"id\": 1, \"status\": \"active\"}\\n')\n        f.write('{\"id\": 2, \"status\": \"pending\"}\\n')\n\n# %%\n# Mock the client\nclient = mock.MagicMock(spec=bigquery.Client)\n\ntable_id = \"project.dataset.json_table\"\njob_config = bigquery.LoadJobConfig(\n    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n    autodetect=True,\n)\n\n# %%\n# Load NDJSON to BigQuery\nwith open(ndjson_path, \"rb\") as source_file:\n    job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n    job.result()\n\nprint(f\"Mock BigQuery JSON load (NDJSON) triggered for {table_id}\")",
    "language": "python",
    "output": "Mock BigQuery JSON load (NDJSON) triggered for project.dataset.json_table\n"
  }
}