{
  "load_csv_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### CSV Loading with Pandas\n# This example demonstrates how to load a CSV file, with self-healing data generation.\n\n# %%\nimport pandas as pd\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.csv\")\n\n# Self-healing: Download if missing\nif not DATA_PATH.exists():\n    urllib.request.urlretrieve(CSV_URL, DATA_PATH)\n\n# %%\n# Basic CSV loading\n# We use the standardized penguins.csv dataset\ndf = pd.read_csv(DATA_PATH)\nprint(f\"Pandas loaded {len(df)} rows:\")\nprint(df.head())\n\n# With options (Selective columns and handling NaNs)\ndf = pd.read_csv(\n    DATA_PATH,\n    usecols=[\"species\", \"island\", \"bill_length_mm\"],\n    na_values=[\"NA\"],\n)\nprint(\"\\nSelective columns:\")\nprint(df.head())",
    "language": "python",
    "output": "Pandas loaded 344 rows:\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    MALE\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  FEMALE\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  FEMALE\n3  Adelie  Torgersen             NaN  ...                NaN          NaN     NaN\n4  Adelie  Torgersen            36.7  ...              193.0       3450.0  FEMALE\n\n[5 rows x 7 columns]\n\nSelective columns:\n  species     island  bill_length_mm\n0  Adelie  Torgersen            39.1\n1  Adelie  Torgersen            39.5\n2  Adelie  Torgersen            40.3\n3  Adelie  Torgersen             NaN\n4  Adelie  Torgersen            36.7\n"
  },
  "load_csv_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n# ]\n# ///\n# %% [markdown]\n# ### CSV Loading with Polars\n# Demonstrates high-performance CSV loading.\n\n# %%\nimport polars as pl\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.csv\")\n\n# Self-healing: Download if missing\nif not DATA_PATH.exists():\n    urllib.request.urlretrieve(CSV_URL, DATA_PATH)\n\n# %%\n# Basic CSV loading (eager)\n# Polars is extremely fast at CSV parsing\ndf = pl.read_csv(DATA_PATH)\nprint(\"Polars loaded penguins dataset:\")\nprint(df.head())\n\n# Lazy loading (recommended for large files)\nlf = pl.scan_csv(DATA_PATH)\ndf = lf.collect()",
    "language": "python",
    "output": "Polars loaded penguins dataset:\nshape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2506 bill_length_mm \u2506 bill_depth_mm \u2506 flipper_length_mm \u2506 body_mass_g \u2506 sex    \u2502\n\u2502 ---     \u2506 ---       \u2506 ---            \u2506 ---           \u2506 ---               \u2506 ---         \u2506 ---    \u2502\n\u2502 str     \u2506 str       \u2506 f64            \u2506 f64           \u2506 i64               \u2506 i64         \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2506 39.1           \u2506 18.7          \u2506 181               \u2506 3750        \u2506 MALE   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 39.5           \u2506 17.4          \u2506 186               \u2506 3800        \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 40.3           \u2506 18.0          \u2506 195               \u2506 3250        \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 null           \u2506 null          \u2506 null              \u2506 null        \u2506 null   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 36.7           \u2506 19.3          \u2506 193               \u2506 3450        \u2506 FEMALE \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "load_csv_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### CSV Loading with DuckDB\n# Using SQL to query CSV files directly.\n\n# %%\nimport duckdb\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.csv\")\n\n# Self-healing: Download if missing\nif not DATA_PATH.exists():\n    urllib.request.urlretrieve(CSV_URL, DATA_PATH)\n\n# %%\n# Basic CSV loading via SQL\n# DuckDB can query CSV files directly\nprint(\"DuckDB querying penguins via SQL:\")\nduckdb.sql(f\"SELECT * FROM '{DATA_PATH}' LIMIT 5\").show()",
    "language": "python",
    "output": "DuckDB querying penguins via SQL:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502 bill_length_mm \u2502 bill_depth_mm \u2502 flipper_length_mm \u2502 body_mass_g \u2502   sex   \u2502\n\u2502 varchar \u2502  varchar  \u2502     double     \u2502    double     \u2502       int64       \u2502    int64    \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502           39.1 \u2502          18.7 \u2502               181 \u2502        3750 \u2502 MALE    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           39.5 \u2502          17.4 \u2502               186 \u2502        3800 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           40.3 \u2502          18.0 \u2502               195 \u2502        3250 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           NULL \u2502          NULL \u2502              NULL \u2502        NULL \u2502 NULL    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           36.7 \u2502          19.3 \u2502               193 \u2502        3450 \u2502 FEMALE  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
  },
  "load_csv_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### CSV Loading with BigQuery\n# Demonstrates loading local data to BigQuery.\n\n# %%\nimport unittest.mock as mock\nfrom google.cloud import bigquery\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.csv\")\n\n# Self-healing: Download if missing\nif not DATA_PATH.exists():\n    urllib.request.urlretrieve(CSV_URL, DATA_PATH)\n\n# %%\n# Mock the client for CI/verification purposes\nclient = mock.MagicMock(spec=bigquery.Client)\n\n# Initialize client (Mocked for verification)\n# client = bigquery.Client()\n\n# Load CSV from local file to BigQuery table\ntable_id = \"project.dataset.penguins\"\n\njob_config = bigquery.LoadJobConfig(\n    source_format=bigquery.SourceFormat.CSV,\n    skip_leading_rows=1,\n    autodetect=True,\n)\n\n# %%\n# Mocking the load_table_from_file behavior\nwith open(DATA_PATH, \"rb\") as source_file:\n    job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n    job.result()  # Wait for the job to complete\n\nprint(f\"Mock BigQuery load triggered for {table_id}\")",
    "language": "python",
    "output": "Mock BigQuery load triggered for project.dataset.penguins\n"
  },
  "load_parquet_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\n# %% [markdown]\n# ### Parquet Loading with Pandas\n# Columnar data handling with PyArrow engine.\n\n# %%\nimport pandas as pd\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.parquet\")\n\n# Self-healing: Download and convert if missing\nif not DATA_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    pd.read_csv(csv_temp).to_parquet(DATA_PATH)\n\n# %%\n# Load Parquet\n# Pandas uses the pyarrow engine by default for Parquet\ndf = pd.read_parquet(DATA_PATH)\nprint(f\"Pandas loaded penguins Parquet with {len(df)} rows.\")\nprint(df.head())",
    "language": "python",
    "output": "Pandas loaded penguins Parquet with 344 rows.\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    MALE\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  FEMALE\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  FEMALE\n3  Adelie  Torgersen             NaN  ...                NaN          NaN     NaN\n4  Adelie  Torgersen            36.7  ...              193.0       3450.0  FEMALE\n\n[5 rows x 7 columns]\n"
  },
  "load_parquet_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\n# %% [markdown]\n# ### Parquet Loading with Polars\n# Fast, multithreaded Parquet reading.\n\n# %%\nimport polars as pl\nimport pathlib\nimport urllib.request\nimport pandas as pd # For initial conversion\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.parquet\")\n\n# Self-healing: Download and convert if missing\nif not DATA_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    pd.read_csv(csv_temp).to_parquet(DATA_PATH)\n\n# %%\n# Load Parquet (Eager)\n# Polars is built on Apache Arrow for ultra-fast Parquet performance\ndf = pl.read_parquet(DATA_PATH)\nprint(\"Polars loaded penguins Parquet:\")\nprint(df.head())\n\n# Scan Parquet (Lazy - Recommended for Large Data)\n# This allows Polars to skip reading columns/rows not needed\nquery = pl.scan_parquet(DATA_PATH).select([\"species\", \"island\"]).limit(5)\nprint(\"\\nLazy scan result:\")\nprint(query.collect())",
    "language": "python",
    "output": "Polars loaded penguins Parquet:\nshape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2506 bill_length_mm \u2506 bill_depth_mm \u2506 flipper_length_mm \u2506 body_mass_g \u2506 sex    \u2502\n\u2502 ---     \u2506 ---       \u2506 ---            \u2506 ---           \u2506 ---               \u2506 ---         \u2506 ---    \u2502\n\u2502 str     \u2506 str       \u2506 f64            \u2506 f64           \u2506 f64               \u2506 f64         \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2506 39.1           \u2506 18.7          \u2506 181.0             \u2506 3750.0      \u2506 MALE   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 39.5           \u2506 17.4          \u2506 186.0             \u2506 3800.0      \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 40.3           \u2506 18.0          \u2506 195.0             \u2506 3250.0      \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 null           \u2506 null          \u2506 null              \u2506 null        \u2506 null   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 36.7           \u2506 19.3          \u2506 193.0             \u2506 3450.0      \u2506 FEMALE \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nLazy scan result:\nshape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2502\n\u2502 ---     \u2506 ---       \u2502\n\u2502 str     \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2502\n\u2502 Adelie  \u2506 Torgersen \u2502\n\u2502 Adelie  \u2506 Torgersen \u2502\n\u2502 Adelie  \u2506 Torgersen \u2502\n\u2502 Adelie  \u2506 Torgersen \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "load_parquet_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\n# %% [markdown]\n# ### Parquet Loading with DuckDB\n# Direct SQL queries on Parquet files.\n\n# %%\nimport duckdb\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.parquet\")\n\n# Self-healing: Download and convert if missing\nif not DATA_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    # Use DuckDB itself to convert CSV to Parquet\n    duckdb.sql(f\"COPY (SELECT * FROM read_csv_auto('{csv_temp}')) TO '{DATA_PATH}' (FORMAT PARQUET)\")\n\n# %%\n# Query Parquet directly via SQL\n# DuckDB treats Parquet files as virtual tables\nprint(\"DuckDB querying penguins Parquet via SQL:\")\nduckdb.sql(f\"SELECT species, island FROM '{DATA_PATH}' LIMIT 5\").show()\n\n# Read as relation\nrel = duckdb.read_parquet(str(DATA_PATH))\nprint(\"\\nRead as relation (First 5):\")\nrel.limit(5).show()",
    "language": "python",
    "output": "DuckDB querying penguins Parquet via SQL:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502\n\u2502 varchar \u2502  varchar  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\nRead as relation (First 5):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502 bill_length_mm \u2502 bill_depth_mm \u2502 flipper_length_mm \u2502 body_mass_g \u2502   sex   \u2502\n\u2502 varchar \u2502  varchar  \u2502     double     \u2502    double     \u2502      double       \u2502   double    \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502           39.1 \u2502          18.7 \u2502             181.0 \u2502      3750.0 \u2502 MALE    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           39.5 \u2502          17.4 \u2502             186.0 \u2502      3800.0 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           40.3 \u2502          18.0 \u2502             195.0 \u2502      3250.0 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           NULL \u2502          NULL \u2502              NULL \u2502        NULL \u2502 NULL    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           36.7 \u2502          19.3 \u2502             193.0 \u2502      3450.0 \u2502 FEMALE  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
  },
  "load_parquet_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\n# %% [markdown]\n# ### Parquet Loading with BigQuery\n# Demonstrates loading Parquet data into BigQuery tables.\n\n# %%\nimport unittest.mock as mock\nfrom google.cloud import bigquery\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.parquet\")\n\n# Self-healing: Download and convert if missing (Local mock data)\nif not DATA_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    import pandas as pd # Needed for conversion\n    pd.read_csv(csv_temp).to_parquet(DATA_PATH)\n\n# %%\n# Mock the client\nclient = mock.MagicMock(spec=bigquery.Client)\n\n# Load configuration\ntable_id = \"project.dataset.penguins_parquet\"\njob_config = bigquery.LoadJobConfig(\n    source_format=bigquery.SourceFormat.PARQUET,\n    write_disposition=\"WRITE_TRUNCATE\",\n)\n\n# %%\n# Trigger load\n# Parquet is the recommended format for BigQuery due to schema persistence\nwith open(DATA_PATH, \"rb\") as source_file:\n    job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n    job.result()\n\nprint(f\"Mock BigQuery Parquet load triggered for {table_id}\")",
    "language": "python",
    "output": "Mock BigQuery Parquet load triggered for project.dataset.penguins_parquet\n"
  },
  "load_json_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### JSON & NDJSON Loading with Pandas\n# Handling standard JSON and Newline Delimited JSON (JSONL).\n\n# %%\nimport pandas as pd\nimport pathlib\nimport urllib.request\nimport json\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nJSON_PATH = pathlib.Path(\"penguins.json\")\nNDJSON_PATH = pathlib.Path(\"penguins.jsonl\")\n\n# Self-healing: Download and convert if missing\nif not JSON_PATH.exists() or not NDJSON_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    df_temp = pd.read_csv(csv_temp)\n    \n    # Save as standard JSON (Array of objects)\n    df_temp.to_json(JSON_PATH, orient=\"records\")\n    \n    # Save as NDJSON (Newline Delimited)\n    df_temp.to_json(NDJSON_PATH, orient=\"records\", lines=True)\n\n# %%\n# Load standard JSON\ndf_json = pd.read_json(JSON_PATH)\nprint(\"Standard JSON (Pandas):\")\nprint(df_json.head())\n\n# %%\n# Load NDJSON (lines=True)\ndf_ndjson = pd.read_json(NDJSON_PATH, lines=True)\nprint(\"\\nNDJSON (lines=True):\")\nprint(df_ndjson.head())",
    "language": "python",
    "output": "Standard JSON (Pandas):\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    MALE\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  FEMALE\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  FEMALE\n3  Adelie  Torgersen             NaN  ...                NaN          NaN     NaN\n4  Adelie  Torgersen            36.7  ...              193.0       3450.0  FEMALE\n\n[5 rows x 7 columns]\n\nNDJSON (lines=True):\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    MALE\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  FEMALE\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  FEMALE\n3  Adelie  Torgersen             NaN  ...                NaN          NaN     NaN\n4  Adelie  Torgersen            36.7  ...              193.0       3450.0  FEMALE\n\n[5 rows x 7 columns]\n"
  },
  "load_json_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n# ]\n# ///\n# %% [markdown]\n# ### JSON & NDJSON Loading with Polars\n# Fast parsing for structured and semi-structured data.\n\n# %%\nimport polars as pl\nimport pathlib\nimport urllib.request\nimport json\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nJSON_PATH = pathlib.Path(\"penguins.json\")\nNDJSON_PATH = pathlib.Path(\"penguins.jsonl\")\n\n# Self-healing: Download and convert if missing\nif not JSON_PATH.exists() or not NDJSON_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    \n    # Use Polars itself for high-performance conversion\n    df_temp = pl.read_csv(csv_temp)\n    \n    # Save as standard JSON (Array of objects)\n    df_temp.write_json(JSON_PATH)\n    \n    # Save as NDJSON (Newline Delimited)\n    df_temp.write_ndjson(NDJSON_PATH)\n\n# %%\n# Load standard JSON (Eager)\n# Polars parses standard JSON into memory efficiently\ndf_json = pl.read_json(JSON_PATH)\nprint(\"Standard JSON (Polars):\")\nprint(df_json.head())\n\n# %%\n# Load NDJSON (Fastest)\n# Newline Delimited JSON is the preferred format for high-speed Polars loading\ndf_ndjson = pl.read_ndjson(NDJSON_PATH)\nprint(\"\\nNDJSON (Polars):\")\nprint(df_ndjson.head())\n\n# Scan NDJSON (Lazy - Great for large logs)\n# df_lazy = pl.scan_ndjson(ndjson_path).collect()",
    "language": "python",
    "output": "Standard JSON (Polars):\nshape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2506 bill_length_mm \u2506 bill_depth_mm \u2506 flipper_length_mm \u2506 body_mass_g \u2506 sex    \u2502\n\u2502 ---     \u2506 ---       \u2506 ---            \u2506 ---           \u2506 ---               \u2506 ---         \u2506 ---    \u2502\n\u2502 str     \u2506 str       \u2506 f64            \u2506 f64           \u2506 f64               \u2506 f64         \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2506 39.1           \u2506 18.7          \u2506 181.0             \u2506 3750.0      \u2506 MALE   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 39.5           \u2506 17.4          \u2506 186.0             \u2506 3800.0      \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 40.3           \u2506 18.0          \u2506 195.0             \u2506 3250.0      \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 null           \u2506 null          \u2506 null              \u2506 null        \u2506 null   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 36.7           \u2506 19.3          \u2506 193.0             \u2506 3450.0      \u2506 FEMALE \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nNDJSON (Polars):\nshape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2506 bill_length_mm \u2506 bill_depth_mm \u2506 flipper_length_mm \u2506 body_mass_g \u2506 sex    \u2502\n\u2502 ---     \u2506 ---       \u2506 ---            \u2506 ---           \u2506 ---               \u2506 ---         \u2506 ---    \u2502\n\u2502 str     \u2506 str       \u2506 f64            \u2506 f64           \u2506 f64               \u2506 f64         \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2506 39.1           \u2506 18.7          \u2506 181.0             \u2506 3750.0      \u2506 MALE   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 39.5           \u2506 17.4          \u2506 186.0             \u2506 3800.0      \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 40.3           \u2506 18.0          \u2506 195.0             \u2506 3250.0      \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 null           \u2506 null          \u2506 null              \u2506 null        \u2506 null   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 36.7           \u2506 19.3          \u2506 193.0             \u2506 3450.0      \u2506 FEMALE \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "load_json_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n# ]\n# ///\n# %% [markdown]\n# ### JSON Loading with DuckDB\n# Direct SQL queries on JSON and NDJSON files.\n\n# %%\nimport duckdb\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nJSON_PATH = pathlib.Path(\"penguins.json\")\nNDJSON_PATH = pathlib.Path(\"penguins.jsonl\")\n\n# Self-healing: Download and convert if missing\nif not JSON_PATH.exists() or not NDJSON_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    # Use DuckDB itself to convert CSV to JSON and NDJSON\n    duckdb.sql(f\"COPY (SELECT * FROM read_csv_auto('{csv_temp}')) TO '{JSON_PATH}' (FORMAT JSON, ARRAY TRUE)\")\n    duckdb.sql(f\"COPY (SELECT * FROM read_csv_auto('{csv_temp}')) TO '{NDJSON_PATH}' (FORMAT JSON)\")\n\n# %%\n# Query JSON directly via SQL\n# DuckDB treats JSON files as virtual tables with auto-schema detection\nprint(\"Standard JSON via SQL:\")\nduckdb.sql(f\"SELECT species, island, island FROM read_json_auto('{JSON_PATH}') LIMIT 5\").show()\n\n# Query NDJSON\nprint(\"\\nNDJSON via SQL:\")\nduckdb.sql(f\"SELECT * FROM read_json_auto('{NDJSON_PATH}') LIMIT 5\").show()",
    "language": "python",
    "output": "Standard JSON via SQL:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502  island   \u2502\n\u2502 varchar \u2502  varchar  \u2502  varchar  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502 Torgersen \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\nNDJSON via SQL:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502 bill_length_mm \u2502 bill_depth_mm \u2502 flipper_length_mm \u2502 body_mass_g \u2502   sex   \u2502\n\u2502 varchar \u2502  varchar  \u2502     double     \u2502    double     \u2502      double       \u2502   double    \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502           39.1 \u2502          18.7 \u2502             181.0 \u2502      3750.0 \u2502 MALE    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           39.5 \u2502          17.4 \u2502             186.0 \u2502      3800.0 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           40.3 \u2502          18.0 \u2502             195.0 \u2502      3250.0 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           NULL \u2502          NULL \u2502              NULL \u2502        NULL \u2502 NULL    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           36.7 \u2502          19.3 \u2502             193.0 \u2502      3450.0 \u2502 FEMALE  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
  },
  "load_json_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### JSON Loading with BigQuery\n# Loading NDJSON data into BigQuery tables. Note: BigQuery requires NDJSON (Newline Delimited) for loading local files.\n\n# %%\nimport unittest.mock as mock\nfrom google.cloud import bigquery\nimport pathlib\nimport urllib.request\nimport pandas as pd # For initial conversion\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nNDJSON_PATH = pathlib.Path(\"penguins.jsonl\")\n\n# Self-healing: Download and convert to NDJSON (Native to BigQuery load)\nif not NDJSON_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    pd.read_csv(csv_temp).to_json(NDJSON_PATH, orient=\"records\", lines=True)\n\n# %%\n# Mock the client\nclient = mock.MagicMock(spec=bigquery.Client)\n\ntable_id = \"project.dataset.penguins_json\"\njob_config = bigquery.LoadJobConfig(\n    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n    autodetect=True,\n)\n\n# %%\n# Load NDJSON to BigQuery\n# BigQuery requires Newline Delimited JSON (NDJSON) for direct file loads\nwith open(NDJSON_PATH, \"rb\") as source_file:\n    job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n    job.result()\n\nprint(f\"Mock BigQuery JSON load (NDJSON) triggered for {table_id}\")",
    "language": "python",
    "output": "Mock BigQuery JSON load (NDJSON) triggered for project.dataset.penguins_json\n"
  }
}